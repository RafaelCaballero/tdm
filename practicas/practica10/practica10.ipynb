{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 9\n",
    "### Tratamiento de datos masivos\n",
    "\n",
    "Partimos del fichero p9data.csv que tiene los datos obtenidos a partir de los sensores del móvil en el bolsillo de personas que han hecho distintos ejercicios, además de algunos datos personales.\n",
    "\n",
    "0 -> subir escaleras\n",
    "\n",
    "1 -> bajarlas\n",
    "\n",
    "2 -> sentarse\n",
    "\n",
    "3 -> ponerse de pie\n",
    "\n",
    "4 -> andar \n",
    "\n",
    "5 -> correr\n",
    "\n",
    "El objetivo de esta práctica es intentar predecir el tipo de movimiento **sin usar la etiqueta (label)**. Para ello se pide elaborar un modelo de clustering basado en kmeans tal y como se especifica a continuación.\n",
    "\n",
    "Enlaces con documentación:\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/ml-clustering.html\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparado!!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "# cambiamos las variables del sistema\n",
    "spark = 'C:\\\\hlocal\\\\tdm\\\\spark\\\\hadoop\\\\spark-2.3.2-bin-hadoop2.7'\n",
    "# en el path se añade\n",
    "path = os.environ.get('PATH') \n",
    "path = path+ ';'+spark+'\\\\bin;'\n",
    "os.environ['PATH'] = path\n",
    "os.environ['SPARK_HOME']= spark \n",
    "os.environ['HADOOP_HOME']= spark \n",
    "os.environ['PYSPARK_DRIVER_PYTHON']= 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']='notebook'\n",
    "\n",
    "# si da problema con collect quizás haya que poner java_home\n",
    "os.environ['JAVA_HOME']= 'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_151'\n",
    "#labs = 'C:\\\\JDK\\\\jdk8-64bits'\n",
    "#os.environ['JAVA_HOME']= labs\n",
    "os.environ['PATH'] = os.environ.get('JAVA_HOME')+'\\\\bin;'+path\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(\"Preparado!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** Cargar el fichero p9data.csv, y dejarlo en una variable de nombre raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = 'c:/hlocal/tdm/p9data.csv'\n",
    "path = 'c:/hlocal/tdm/p9data.csv'\n",
    "\n",
    "## Solución\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- attitude_roll: integer (nullable = true)\n",
      " |-- attitude_pitch: double (nullable = true)\n",
      " |-- attitude_yaw: double (nullable = true)\n",
      " |-- gravity_x: double (nullable = true)\n",
      " |-- gravity_y: double (nullable = true)\n",
      " |-- gravity_z: double (nullable = true)\n",
      " |-- rotationRate_x: double (nullable = true)\n",
      " |-- rotationRate_y: double (nullable = true)\n",
      " |-- rotationRate_z: double (nullable = true)\n",
      " |-- userAcceleration_x: double (nullable = true)\n",
      " |-- userAcceleration_y: double (nullable = true)\n",
      " |-- userAcceleration_z: double (nullable = true)\n",
      " |-- c: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# para probarlo\n",
    "raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** Convertir todas las columnas salvo 'label' en un vector de nombre 'rawfeatures'. Quedarse solo con las columnas label y feaeuteres. El resultado será un dataset con nombre 'datosprep', y que tendrá el esquema\n",
    "\n",
    "```\n",
    "datosprep.printSchema()\n",
    "root\n",
    " |-- label: integer (nullable = true)\n",
    " |-- rawfeatures: vector (nullable = true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "\n",
    "\n",
    "# para probar:\n",
    "# datosprep.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** Antes de hacer kmeans, hay que escalar los datos. Para esto utilizaremos el transformador MinMaxScaler, que se aplicara como columna de entrada a rawfeatures, y como columna de salida, 'features'. Para hacer esto tendremos que hacer un modelo (al que podemos llamar scalerModel) y aplicarselo al propio dataframe datosprep. El resultado será un dataframe al que podemmos llamar scaledData.\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html#minmaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solución\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|         rawfeatures|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[0.0,1.528132,-0....|[0.0,0.7432113863...|\n",
      "|    0|[1.0,1.527992,-0....|[6.08939227865059...|\n",
      "|    0|[2.0,1.527765,-0....|[1.21787845573011...|\n",
      "|    0|[4.0,1.493941,-0....|[2.43575691146023...|\n",
      "|    0|[5.0,1.476302,-0....|[3.04469613932529...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# para probarlo\n",
    "scaledData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4** Crear una variable model para el número de clusters k que se desee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# esto no lo tocamos\n",
    "k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras obtener el modelo vemos el resultado, aplicándoselo al propio conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|         rawfeatures|            features|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|    0|[0.0,1.528132,-0....|[0.0,0.7432113863...|         7|\n",
      "|    0|[1.0,1.527992,-0....|[6.08939227865059...|         7|\n",
      "|    0|[2.0,1.527765,-0....|[1.21787845573011...|         7|\n",
      "|    0|[4.0,1.493941,-0....|[2.43575691146023...|         7|\n",
      "|    0|[5.0,1.476302,-0....|[3.04469613932529...|         7|\n",
      "|    0|[6.0,1.455153,-0....|[3.65363536719035...|         7|\n",
      "|    0|[8.0,1.44344,-0.6...|[4.87151382292047...|         7|\n",
      "|    0|[9.0,1.443071,-0....|[5.48045305078553...|         7|\n",
      "|    0|[10.0,1.434186,-0...|[6.08939227865059...|         7|\n",
      "|    0|[12.0,1.366432,-0...|[7.30727073438070...|         7|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicho = model.transform(scaledData)\n",
    "predicho.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5** [4 puntos] Ahora queremos saber, para cada cluster (entre 0 y k-1), cuantos valores de cada valor para label entre 0 y 5 (ambos inclusive) hay. Dada una etiqueta l y un cluster i, podemos contar el número de elementos etiquetados como l en i mediante\n",
    "```[python]\n",
    "predicho.where('label=='+str(l)+' and prediction=='+str(i)).count()\n",
    "```\n",
    "El resultado de este paso será un array de arrays, con nombre t, con la forma (para k = 7):\n",
    "```[Python]\n",
    "[[0, 0, 38858, 35382, 39264, 3135], \n",
    " [36032, 38998, 35798, 35808, 50736, 24069], \n",
    " [0, 0, 22608, 18246, 0, 0], \n",
    " [0, 0, 41845, 34768, 7406, 0], \n",
    " [47962, 48748, 32880, 32880, 47265, 32677], \n",
    " [294, 3172, 40853, 34688, 50374, 19513], \n",
    " [14631, 27073, 41258, 38068, 51989, 21297]]\n",
    "```\n",
    "\n",
    "Nota: Estos son valores de un ejemplo ¡no tienen que salir los mismos! (kmeans tiene un componente aleatorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9713, 12231, 197, 8314, 18590, 16094]\n",
      "[19958, 23058, 8644, 25954, 90720, 22937]\n",
      "[1992, 1692, 74740, 0, 939, 534]\n",
      "[387, 2966, 166712, 0, 8941, 2601]\n",
      "[1536, 1637, 3689, 2407, 2224, 1930]\n",
      "[17271, 14619, 38, 30435, 28399, 15977]\n",
      "[9127, 11730, 5, 86575, 28088, 10396]\n",
      "[19906, 25213, 3, 36297, 33961, 13476]\n",
      "[12624, 12945, 3, 17194, 20400, 10775]\n",
      "[6405, 11900, 69, 22664, 14772, 5971]\n",
      "[[9713, 12231, 197, 8314, 18590, 16094], [19958, 23058, 8644, 25954, 90720, 22937], [1992, 1692, 74740, 0, 939, 534], [387, 2966, 166712, 0, 8941, 2601], [1536, 1637, 3689, 2407, 2224, 1930], [17271, 14619, 38, 30435, 28399, 15977], [9127, 11730, 5, 86575, 28088, 10396], [19906, 25213, 3, 36297, 33961, 13476], [12624, 12945, 3, 17194, 20400, 10775], [6405, 11900, 69, 22664, 14772, 5971]]\n"
     ]
    }
   ],
   "source": [
    "# Solución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6**  Si tuvieras que elegir con el valor k=10, un clúster para detectar lo mejor posible si alguien está sentándose (label==2)\n",
    "¿qué número de clúster elegirías? Razona tu respuesta (y no la discutas con tu compañero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solución"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
